{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRCNN Using Theano\n",
    "\n",
    "This is a notebook on theano based Image-super resolution based on this [paper](https://arxiv.org/abs/1501.00092).\n",
    "This implementation is based on the implementation of ['corochann'](https://github.com/corochann).\n",
    "The original codes of this implementation can be found [here](https://github.com/corochann/theanonSR).\n",
    "The images used have been taken from [here](https://github.com/jbhuang0604/SelfExSR)\n",
    "This is all work of the original authors of the papers. The purpose of this notebook is only to make the process easier to comprehend and give a toy example of how to implement it.\n",
    "\n",
    "## Introduction to Image Super_resolution\n",
    "\n",
    "Single image super-resolution (SR), which aims at recovering a high-resolution image from a single low-resolution image, is a classical problem in computer vision. This problem is inherently ill-posed since a multiplicity of solutions exist for any given low-resolution pixel.\n",
    "\n",
    "Such a problem is typically mitigated by constraining the solution space by strong prior information. To learn the prior,recent state-of-the-art methods mostly adopt example-based strategy. These methods either exploit internal similarities of the same image or learn mappingfunctions from external low- and high-resolution exemplar pairs.\n",
    "\n",
    "In this paper a Deep Convolutional Neural Network has been developed to solve this problem.The proposed Super-Resolution-convoluted-neural-network, SRCNN, has several appealing properties. First, its structure is intentionally designed with simplicity in mind, and yet provides superior accuracy.Secondly, with morderate numbers of filters and layers, this method achieves fast speed for practical on-line usage even on a CPU.\n",
    "\n",
    "## The model\n",
    "\n",
    "Consider a single low-resolution image, we first upscale it to the desired size using bicubic interpolation, which is the only pre-processing we perform. Let us denote the interpolated image as $Y$. Our goal is to recover from $Y$ an image $F(Y)$ that is as similaras possible to the ground truth high-resolution image $X$. For the ease of presentation, we still call $Y$ a “low-resolution” image, although it has the same size as $X$. We wish to learn a mapping $F$, which conceptually consists of three operations: \n",
    "\n",
    "1) <b>Patch extraction and representation:</b> this operation extracts (overlapping) patches from the low-resolution image $Y$ and represents each patch as a high-dimensional vector. These vectors comprise a set of feature maps, of which the number equals to the dimensionality of the vectors. \n",
    "\n",
    "2) <b>Non-linear mapping:</b> this operation nonlinearly maps each high-dimensional vector onto another high-dimensional vector. Each mapped vector is conceptually the representation of a high-resolution patch. These vectors comprise another set of feature maps. \n",
    "\n",
    "3) <b>Reconstruction:</b> this operation aggregates the above high-resolution patch-wise representations to generate the final high-resolution image. This image is expected to be similar to the ground truth $X$. \n",
    "\n",
    "\n",
    "\n",
    "![model image](model.png)\n",
    "\n",
    "\n",
    "This can be mathematically be represented as\n",
    "\n",
    "1) \n",
    "\\begin{equation}\n",
    "F_1 (Y) = max(0,W_1*Y +B_1)\n",
    "\\end{equation}\n",
    "where $W_1$ and $B_1$ represent the filters and biases respectively, and ’*’ denotes the convolution operation. Here,$ W_1$ corresponds to $n_1$ filters of support $c\\times f_1 \\times f_1$, where $c$ is the number of channels in the input image, $f_1$ is the spatial size of a filter.\n",
    "\n",
    "2) \n",
    "\\begin{equation}\n",
    "F_2 (Y) = max(0,W_2*F_1(Y) +B_2)\n",
    "\\end{equation}\n",
    "Here, $W_2$ contains $n_2$ filters of size $n_1\\times f_2 \\times f_2$, and $B_2$ is $n_2$-dimensional.\n",
    "\n",
    "3) \n",
    "\\begin{equation}\n",
    "F(Y) = W_3*F_2(Y) +B_3\n",
    "\\end{equation}\n",
    "Here $W_3$ corresponds to $c$ filters of a size $n_2 \\times f_3\\times f_3$, and $B_3$ is a $c$-dimensional vector.\n",
    "\n",
    "## Training\n",
    "\n",
    "Learning the end-to-end mapping function $F$ requires the estimation of network parameters $\\Theta = \\{ W_1;W_2;W_3;B_1;B_2;B_3\\} $. This is achieved through min-imizing the loss between the reconstructed images $F(Y; \\Theta)$ and the corresponding ground truth high-resolution images $X$. Given a set of high-resolution images $\\{Xi\\}$ and their corresponding low-resolution images $\\{Yi\\}$, we use Mean Squared Error (MSE) as the loss function: \n",
    "\n",
    "\\begin{equation}\n",
    "L(\\Theta) = \\frac{1}{n} \\sum_{i=1}^{n}{{|| F(Y_i,\\Theta) - X_i ||}^2},\n",
    "\\end{equation}\n",
    "where $n$ is the number of training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load the data files\n",
    "import sys\n",
    "import os\n",
    "h_r_folder_name = 'image-dataset/h_r'\n",
    "l_r_folder_name = 'image-dataset/l_r'\n",
    "h_r_files_list = os.listdir(h_r_folder_name)\n",
    "l_r_files_list = os.listdir(l_r_folder_name)\n",
    "h_r_files_list = [h_r_folder_name+'/'+ x for x in h_r_files_list]\n",
    "l_r_files_list = [l_r_folder_name+'/'+ x for x in l_r_files_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets see some files\n",
    "import numpy as np\n",
    "import cv2\n",
    "img_hr = cv2.imread(h_r_files_list[3])\n",
    "img_lr = cv2.imread(l_r_files_list[3])\n",
    "cv2.imshow('image',img_lr)\n",
    "cv2.imshow('image2',img_hr)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now if we apply the Ycbcr tranformation, they would be converted to the other color system\n",
    "# which would be (wrongly) represented in the RGB color system as the follow\n",
    "img_lr = cv2.cvtColor(img_lr, cv2.COLOR_BGR2YCR_CB)\n",
    "img_hr = cv2.cvtColor(img_hr, cv2.COLOR_BGR2YCR_CB)\n",
    "cv2.imshow('image',img_lr)\n",
    "cv2.imshow('image2',img_hr)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we need to zoom the low resolution image using bicubic interpolation\n",
    "# and convert them to Ycbcr format\n",
    "# Along with that we need to make subsample pairs\n",
    "\n",
    "no_of_files = len(h_r_files_list)\n",
    "\n",
    "## For subsampling, let the subsamples have the image size 80px*80px\n",
    "subimg_length = 32\n",
    "stride_length = 32\n",
    "stride_height = 32\n",
    "subimg_height = 32\n",
    "\n",
    "sample_img = cv2.imread(h_r_files_list[0])\n",
    "subimg_for_length = int(np.shape(sample_img)[0])/(subimg_length+stride_length)\n",
    "subimg_for_height = int(np.shape(sample_img)[1])/(subimg_height+stride_height)\n",
    "\n",
    "# Final arrays \n",
    "X = np.ndarray(shape = (no_of_files*subimg_for_length*subimg_for_height,subimg_length,subimg_height,3))\n",
    "Y = np.ndarray(shape = (no_of_files*subimg_for_length*subimg_for_height,subimg_length,subimg_height,3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(no_of_files):\n",
    "    # Zooming the images\n",
    "    img_lr = cv2.imread(l_r_files_list[i])\n",
    "    img_lr_zoomed = cv2.resize(img_lr, None,fx=2,fy=2, interpolation = cv2.INTER_CUBIC)\n",
    "    img_hr = cv2.imread(h_r_files_list[i])\n",
    "    # Now we need to convert it to YCbCr\n",
    "    img_lr_zoomed = cv2.cvtColor(img_lr_zoomed, cv2.COLOR_BGR2YCR_CB)\n",
    "    img_hr = cv2.cvtColor(img_hr, cv2.COLOR_BGR2YCR_CB)\n",
    "    #img_lr_zoomed.resize(3,480,320)\n",
    "    #img_hr.resize(3,480,320)\n",
    "    # Now we need to make subsamples\n",
    "    for j in range(subimg_for_length):\n",
    "        for k in range(subimg_for_height):\n",
    "            img_current_lr = img_lr_zoomed[j*(stride_length+ subimg_length):j*(stride_length+ subimg_length)+subimg_length,\n",
    "                                           k*(stride_height+ subimg_height):k*(stride_height+ subimg_height)+subimg_height,:]\n",
    "            img_current_hr = img_hr[j*(stride_length+ subimg_length):j*(stride_length+ subimg_length)+subimg_length,\n",
    "                                    k*(stride_height+ subimg_height):k*(stride_height+ subimg_height)+subimg_height,:]\n",
    "            X[i*subimg_for_height*subimg_for_length+j*subimg_for_height +k] = img_current_lr\n",
    "            Y[i*subimg_for_height*subimg_for_length+j*subimg_for_height +k] = img_current_hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step\n",
    "Now that our data is ready, we need to make the Convoluted Neural network. we use Lasagne on top of theano for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Now we have subimages.\n",
    "## Now we can make the 3 layer convolution with filter sized 5,1,3\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "#from layer import ConvLayer\n",
    "#from tools.image_processing import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.tensor4('targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network = lasagne.layers.InputLayer(shape=(None, 3,None, None), input_var=input_var)\n",
    "network = lasagne.layers.Conv2DLayer(\n",
    "        network, num_filters=32,pad=2, filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "network = lasagne.layers.Conv2DLayer(\n",
    "        network, num_filters=32, filter_size=(1, 1),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "network = lasagne.layers.Conv2DLayer(\n",
    "        network,pad=1, num_filters=3, filter_size=(3, 3),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.squared_error(prediction, target_var)\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network,deterministic=True)\n",
    "test_loss = lasagne.objectives.squared_error(test_prediction,target_var)\n",
    "test_loss = test_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss])\n",
    "# for true test set\n",
    "predict_fn = theano.function([input_var],[test_prediction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## We need to change the data shape\n",
    "no_of_imgs = int(np.shape(X)[0])\n",
    "#X.shape(no_of_imgs,3,subimg_length,subimg_height)\n",
    "X = np.reshape(X,(no_of_imgs,3,subimg_length,subimg_height))\n",
    "Y = np.reshape(Y,(no_of_imgs,3,subimg_length,subimg_height))\n",
    "np.shape(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X, Y, 100, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        #val_err = 0\n",
    "        #val_acc = 0\n",
    "        #val_batches = 0\n",
    "        #for batch in iterate_minibatches(X, Y, 100, shuffle=False):\n",
    "        #    inputs, targets = batch\n",
    "        #    err, acc = val_fn(inputs, targets)\n",
    "        #    val_err += err\n",
    "        #    val_acc += acc\n",
    "        #    val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        #print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        #print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        #    val_acc / val_batches * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "Now let us compare the results of bicubic interpolation and SRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we have trained our CNN\n",
    "# Lets compare its peroformance to Bicubical interpolation\n",
    "\n",
    "img_lr = cv2.imread(l_r_files_list[1],1)\n",
    "img_hr_bicubic = cv2.resize(img_lr, None,fx=2,fy=2, interpolation = cv2.INTER_CUBIC)\n",
    "cv2.imshow('image2',img_hr_bicubic)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_lr = cv2.imread(l_r_files_list[1],1)\n",
    "img_hr_bicubic = cv2.resize(img_lr, None,fx=2,fy=2, interpolation = cv2.INTER_CUBIC)\n",
    "img_length = int(np.shape(img_hr_bicubic)[0])\n",
    "img_height = int(np.shape(img_hr_bicubic)[1])\n",
    "img_hr_bicubic_ycbcr = cv2.cvtColor(img_hr_bicubic, cv2.COLOR_BGR2YCR_CB)\n",
    "#img_hr_srcnn = img_hr_bicubic_ycbcr\n",
    "img_hr_srcnn = predict_fn(img_hr_bicubic_ycbcr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_hr_srcnn_correct_dim = np.reshape(img_hr_srcnn,(img_length,img_height,3))\n",
    "img_hr_srcnn_correct_dim_BGR = cv2.cvtColor(img_hr_srcnn_correct_dim, cv2.COLOR_YCR_CB2BGR)\n",
    "cv2.imshow('image2',img_hr_srcnn_correct_dim_BGR)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As a typical thrid world problem, I dont have the hardware to train a CNN. However, it seems that we were able to lay out the simple pipeline efficiently.\n",
    "\n",
    "Another major issue is that the model gets trapped at local minimas far from the global minima.However, maybe with proper training rig, such issues can be solved by reinitializing the system.(or maybe we need better initialization than glorat's?)\n",
    "\n",
    "I hope that the entire process of Image Super Resolution through Convoluted Neural Networks is clear to anyone who reads through this notebook. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
